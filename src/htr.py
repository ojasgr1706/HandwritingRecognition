# -*- coding: utf-8 -*-
"""HTR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/120_RuGfrjSxai0RWJS0IlQWLmlaZaR9N
"""

# !git clone --recursive https://github.com/parlance/ctcdecode.git
# !cd ctcdecode && pip install .

import torch
from DataLoader import HTR_Dataset
from Model import Model
from torch.utils.data import DataLoader, random_split
from torch.optim import Adam
from helpers import encode, decode
import tarfile
import pdb

# from ctcdecode import CTCBeamDecoder

data_tar = tarfile.open('data.tar.gz')
data_tar.extractall('./')
data_tar.close()

size = {'small' : 0,'mid' : 1, 'big' : 2}
dataset = size['mid']

if dataset == 0:
  gt = "../data/small/ascii/words.txt"
  data_path = "../data/small/words"
  charlist_path = "../data/small/charList.txt"

elif dataset == 1:
  gt = "../data/mid/ascii/words.txt"
  data_path = "../data/mid/words"
  charlist_path = "../data/mid/charList.txt"

else:
  gt = "../data/big/ascii/words.txt"
  data_path = "../data/big/words"
  charlist_path = "../data/big/charList.txt"

save_path = "../model/model.pth"

device = "cuda" if torch.cuda.is_available() else "cpu"
kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# define the train and val splits
TRAIN_SPLIT = 0.7
VAL_SPLIT = 0.2
TEST_SPLIT = 0.1

# training parameters

INIT_LR = 1e-3
BATCH_SIZE = 64
EPOCHS = 12

# print(len(train_data), len(test_data))

Data = HTR_Dataset(gt, data_path, charlist_path)
# test_data = HTR_Dataset(gt, test_path)

length = len(Data)
numTrainSamples = int(length * TRAIN_SPLIT)
numTestSamples = int(length * TEST_SPLIT)
numValSamples = length - numTrainSamples - numTestSamples
(train_data, test_data, val_data) = random_split(Data, [numTrainSamples, numTestSamples, numValSamples], generator = torch.Generator().manual_seed(42))

# initialize the train, validation, and test data loaders
trainLoader = DataLoader(train_data, shuffle = True, batch_size = BATCH_SIZE, **kwargs)
valLoader = DataLoader(val_data, batch_size = BATCH_SIZE, shuffle = True, **kwargs)
testLoader = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle = True, **kwargs)

# calculate steps per epoch for training and validation set
trainSteps = len(trainLoader.dataset) // BATCH_SIZE
valSteps = len(valLoader.dataset) // BATCH_SIZE

Data.charlist
# print(trainSteps)

len(Data.charlist)

# Training

model = Model()
opt = Adam(model.parameters(), lr=INIT_LR)
lossFn = torch.nn.CTCLoss()
# decoder = CTCBeamDecoder(
#     list(Data.charlist),
#     model_path=None,
#     alpha=0,
#     beta=0,
#     cutoff_top_n=40,
#     cutoff_prob=1.0,
#     beam_width=100,
#     num_processes=2,
#     blank_id=0,
#     log_probs_input=True
# )

H = {
	"train_loss": [],
	"train_acc": [],
	"val_loss": [],
	"val_acc": []
}

train_decode = list()
val_decode = list()

print("[INFO] training the network...")

for epoch in range(EPOCHS):
	print("Epoch :",epoch)
 
	totalTrainLoss = 0
	totalValLoss = 0
	# model = model.float()
	model = model.double()
	model.train()
 

	count = 0
	for (x, y) in trainLoader:
		count += 1
		print("t =",count, "; samples =", x.shape[0])
		# send the input to the device
		# perform a forward pass and calculate the training loss
		target = []
		train_out_len = []
		for i in range(len(y)):
			target.append(encode(y[i],Data.mapping))
			train_out_len.append(len(y[i]))
		target = torch.tensor(target)

		# y = torch.Tensor(y)
		model_out = model(x)
		# pdb.set_trace()

		(x, target) = (x.to(device), target.to(device))
		input_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)
		output_length = torch.tensor(train_out_len)
		# output_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)

		opt.zero_grad()
		# pdb.set_trace()
		train_loss = lossFn(model_out.permute(1,0,2), target, input_length, output_length)
		train_loss.backward()
		opt.step()
	
		totalTrainLoss += train_loss

		print("train loss =",train_loss)
	H['train_loss'].append(totalTrainLoss)


	# beam_results, beam_scores, timesteps, out_lens = decoder.decode(model_out)
	# train_decode.append([beam_results, beam_scores, timesteps, out_lens])
	
	with torch.no_grad():
		# set the model in evaluation mode
		model.eval()
		# loop over the validation set
		for (x, y) in valLoader:
			# send the input to the device
			target = []
			val_out_len = []
			for i in range(len(y)):
				target.append(encode(y[i],Data.mapping))
				val_out_len.append(len(y[i]))
			target = torch.tensor(target)
			(x, target) = (x.to(device), target.to(device))
	
			model_out = model(x)
			input_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)
			output_length = torch.tensor(val_out_len)
			# output_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)
			# make the predictions and calculate the validation loss
			val_loss = lossFn(model_out.permute(1,0,2), target, input_length, output_length)
			totalValLoss += val_loss
		# beam_results, beam_scores, timesteps, out_lens = decoder.decode(model_out)
		# val_decode.append([beam_results, beam_scores, timesteps, out_lens])
		print("val loss =",val_loss)

	H['val_loss'].append(totalValLoss)
	
	 
	if epoch % 3 == 2:
		torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': opt.state_dict()}, save_path)
		print("[INFO] saved model...")


# Retraining (if needed)

# model = Model()
# opt = Adam(model.parameters(), lr=INIT_LR)

# # load checkpoint
# checkpoint = torch.load(save_path)
# model.load_state_dict(checkpoint['model_state_dict'])
# # opt.load_state_dict(checkpoint['optimizer_state_dict'])

# lossFn = torch.nn.CTCLoss()
# decoder = CTCBeamDecoder(
#     list(Data.charlist),
#     model_path=None,
#     alpha=0,
#     beta=0,
#     cutoff_top_n=40,
#     cutoff_prob=1.0,
#     beam_width=100,
#     num_processes=2,
#     blank_id=0,
#     log_probs_input=True
# )

# H = {
# 	"train_loss": [],
# 	"train_acc": [],
# 	"val_loss": [],
# 	"val_acc": []
# }

# train_decode = list()
# val_decode = list()

# print("[INFO] training the network...")

# for epoch in range(EPOCHS):
# 	print("Epoch :",epoch)
 
# 	totalTrainLoss = 0
# 	totalValLoss = 0
# 	# model = model.float()
# 	model = model.double()
# 	model.train()
 

# 	count = 0
# 	for (x, y) in trainLoader:
# 		count += 1
# 		print("batch =",count, "; samples =", x.shape[0])
# 		# send the input to the device
# 		# perform a forward pass and calculate the training loss
# 		target = []
# 		train_out_len = []
# 		for i in range(len(y)):
# 			target.append(encode(y[i],Data.mapping))
# 			train_out_len.append(len(y[i]))
# 		target = torch.tensor(target)

# 		# y = torch.Tensor(y)
# 		model_out = model(x)
# 		# pdb.set_trace()

# 		(x, target) = (x.to(device), target.to(device))
# 		input_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)
# 		output_length = torch.tensor(train_out_len)
# 		# output_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)

# 		opt.zero_grad()
# 		# pdb.set_trace()
# 		train_loss = lossFn(model_out.permute(1,0,2), target, input_length, output_length)
# 		train_loss.backward()
# 		opt.step()
	
# 		totalTrainLoss += train_loss

# 		print("train loss =",train_loss)


	# beam_results, beam_scores, timesteps, out_lens = decoder.decode(model_out)
	# train_decode.append([beam_results, beam_scores, timesteps, out_lens])
# 	H['train_loss'].append(totalTrainLoss)
	
# 	with torch.no_grad():
# 		# set the model in evaluation mode
# 		model.eval()
# 		# loop over the validation set
# 		for (x, y) in valLoader:
# 			# send the input to the device
# 			target = []
# 			val_out_len = []
# 			for i in range(len(y)):
# 				target.append(encode(y[i],Data.mapping))
# 				val_out_len.append(len(y[i]))
# 			target = torch.tensor(target)
# 			(x, target) = (x.to(device), target.to(device))
	
# 			model_out = model(x)
# 			input_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)
# 			output_length = torch.tensor(val_out_len)
# 			# output_length = 32*torch.ones(model_out.shape[0], dtype=torch.int32)
# 			# make the predictions and calculate the validation loss
# 			val_loss = lossFn(model_out.permute(1,0,2), target, input_length, output_length)
# 			totalValLoss += val_loss
		# beam_results, beam_scores, timesteps, out_lens = decoder.decode(model_out)
		# val_decode.append([beam_results, beam_scores, timesteps, out_lens])
# 		H['val_loss'].append(totalValLoss)
	 
# 		print("val loss =",val_loss)
	 
# 	if epoch % 3 == 2:
# 		torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': opt.state_dict()}, save_path)





# Testing

# with torch.no_grad():
#   # set the model in evaluation mode
#   model.eval()
#   # loop over the validation set
#   for (x, y) in testLoader:
#     # send the input to the device
#     target = []

#     for i in range(len(y)):
#       target.append(encode(y[i],Data.mapping))
#     target = torch.tensor(target)
#     (x, target) = (x.to(device), target.to(device))

#     model_out = model(x)
    
#     beam_results, beam_scores, timesteps, out_lens = decoder.decode(model_out)
#     for i in range(len(y)):
#       pred = beam_results[i][0][:out_lens[i][0]]
#       print("ground truth :",y[i],"|| predicted :",decode(pred,Data.mapping))











